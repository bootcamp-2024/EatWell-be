{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nhatn\\.conda\\envs\\gen_meal\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\nhatn\\.conda\\envs\\gen_meal\\lib\\site-packages\\huggingface_hub-0.23.0-py3.8.egg\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DetrImageProcessor, DetrForObjectDetection\n",
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "# you can specify the revision tag if you don't want the timm dependency\n",
    "processor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-101\", revision=\"no_timm\")\n",
    "model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-101\", revision=\"no_timm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://img.freepik.com/free-photo/real-food-pyramid-assortment-top-view_23-2150238927.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['orange', 'apple', 'apple', 'bowl', 'dining table', 'bowl', 'bird', 'apple', 'bowl', 'orange', 'orange', 'banana', 'banana', 'orange', 'bowl']\n"
     ]
    }
   ],
   "source": [
    "# convert outputs (bounding boxes and class logits) to COCO API\n",
    "# let's only keep detections with score > 0.6\n",
    "target_sizes = torch.tensor([image.size[::-1]])\n",
    "results = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.6)[0]\n",
    "\n",
    "detected_objects = []\n",
    "for label in results[\"labels\"]:\n",
    "    detected_objects.append(model.config.id2label[label.item()])\n",
    "print(detected_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nFlaxAutoModelForSeq2SeqLM requires the FLAX library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://github.com/google/flax and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m MODEL_NAME_OR_PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflax-community/t5-recipe-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      5\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(MODEL_NAME_OR_PATH, use_fast\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 6\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mFlaxAutoModelForSeq2SeqLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(MODEL_NAME_OR_PATH)\n\u001b[0;32m      8\u001b[0m prefix \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mitems: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# generation_kwargs = {\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m#     \"max_length\": 512,\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m#     \"min_length\": 64,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m#     \"length_penalty\": 1.5,\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# }\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nhatn\\.conda\\envs\\gen_meal\\lib\\site-packages\\transformers\\utils\\import_utils.py:1304\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[1;34m(cls, key)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[1;32m-> 1304\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nhatn\\.conda\\envs\\gen_meal\\lib\\site-packages\\transformers\\utils\\import_utils.py:1292\u001b[0m, in \u001b[0;36mrequires_backends\u001b[1;34m(obj, backends)\u001b[0m\n\u001b[0;32m   1290\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n\u001b[0;32m   1291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[1;32m-> 1292\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\u001b[1;31mImportError\u001b[0m: \nFlaxAutoModelForSeq2SeqLM requires the FLAX library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://github.com/google/flax and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import FlaxAutoModelForSeq2SeqLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "MODEL_NAME_OR_PATH = \"flax-community/t5-recipe-generation\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME_OR_PATH, use_fast=True)\n",
    "model = FlaxAutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME_OR_PATH)\n",
    "\n",
    "prefix = \"items: \"\n",
    "# generation_kwargs = {\n",
    "#     \"max_length\": 512,\n",
    "#     \"min_length\": 64,\n",
    "#     \"no_repeat_ngram_size\": 3,\n",
    "#     \"early_stopping\": True,\n",
    "#     \"num_beams\": 5,\n",
    "#     \"length_penalty\": 1.5,\n",
    "# }\n",
    "generation_kwargs = {\n",
    "    \"max_length\": 512,\n",
    "    \"min_length\": 64,\n",
    "    \"no_repeat_ngram_size\": 3,\n",
    "    \"do_sample\": True,\n",
    "    \"top_k\": 60,\n",
    "    \"top_p\": 0.95\n",
    "}\n",
    "\n",
    "\n",
    "special_tokens = tokenizer.all_special_tokens\n",
    "tokens_map = {\n",
    "    \"<sep>\": \"--\",\n",
    "    \"<section>\": \"\\n\"\n",
    "}\n",
    "def skip_special_tokens(text, special_tokens):\n",
    "    for token in special_tokens:\n",
    "        text = text.replace(token, \"\")\n",
    "\n",
    "    return text\n",
    "\n",
    "def target_postprocessing(texts, special_tokens):\n",
    "    if not isinstance(texts, list):\n",
    "        texts = [texts]\n",
    "    \n",
    "    new_texts = []\n",
    "    for text in texts:\n",
    "        text = skip_special_tokens(text, special_tokens)\n",
    "\n",
    "        for k, v in tokens_map.items():\n",
    "            text = text.replace(k, v)\n",
    "\n",
    "        new_texts.append(text)\n",
    "\n",
    "    return new_texts\n",
    "\n",
    "def generation_function(texts):\n",
    "    _inputs = texts if isinstance(texts, list) else [texts]\n",
    "    inputs = [prefix + inp for inp in _inputs]\n",
    "    inputs = tokenizer(\n",
    "        inputs, \n",
    "        max_length=256, \n",
    "        padding=\"max_length\", \n",
    "        truncation=True, \n",
    "        return_tensors=\"jax\"\n",
    "    )\n",
    "\n",
    "    input_ids = inputs.input_ids\n",
    "    attention_mask = inputs.attention_mask\n",
    "\n",
    "    output_ids = model.generate(\n",
    "        input_ids=input_ids, \n",
    "        attention_mask=attention_mask,\n",
    "        **generation_kwargs\n",
    "    )\n",
    "    generated = output_ids.sequences\n",
    "    generated_recipe = target_postprocessing(\n",
    "        tokenizer.batch_decode(generated, skip_special_tokens=False),\n",
    "        special_tokens\n",
    "    )\n",
    "    return generated_recipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen_meal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
